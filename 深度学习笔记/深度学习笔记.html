<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>深度学习原理笔记</title>
</head>
<body>
    <h1>目录</h1>
    <a href="https://www.bilibili.com/video/BV1sd4y167NS?p=2&vd_source=8814c55673559083f0880c3918024708">视频资料</a><br/>
    <a href="Book-Mathmatical-Foundation-of-Reinforcement-Learning-Lecture slides 2023.02.20.pdf">PPT</a><br/>
    <h2><a href="#0zero">第0课</a></h2>
    <h2><a href="#1st">第1课</a></h2>
    <h2><a href="#2nd">第2课</a></h2>
    <h2><a href="#3rd">第3课</a></h2>
    <h2><a href="#4th">第4课</a></h2>
    <h2><a href="#5th">第5课</a></h2>
    <h2><a href="#6th">第6课</a></h2>
    <h2><a href="#7th">第7课</a></h2>
    <h2 id="0zero">第0课</h2>
    <p>
        <img src="zero.png"/>
    </p>
    <h2 id="1st">第1课</h2>
    <br>
        1.policy、Π<br/>
        2.reward、trajectory(轨迹)、return<br/>
        3.discount return、episode(trail)、[episodic and continuing tasks]<br/>
        4.MDP:Sets[S,A(s),R(s,a)]、[p(s'|s,a)、p(r|s,a)]<br/>
              Policy[Π(a,s)]<br/>
              Markov/Memoryless <br/>
    </p>
    <h2 id="2nd">第2课</h2>
    <p>
        1.boostrapping、v=r+γPv<br/>
        2.state value<br/>
        3.bellman equation<br/>
        <img src="bellman equation.png"/><br/>
        4.state transition matrix<br/>
        <img src="bellman matrix.png"/><br/>
        5.action value<br/>
        <img src="action value.png"/><br/>
    </p>
    <h2 id="3rd">第3课</h2>
    <p>
        1.BOE<br/>
        <img src="bellman optimality equation.png"/><br/>
        <img src="BOE matrix.png"/><br/>
        2.Fixed point、contraction mapping、contraction mapping theorem<br/>
        <img src="contraction mapping.png"/><br/>
        3.value iteration algorithm<br/>
    </p>
    <h2 id="4th">第4课</h2>
    <P>
        1.value iteration algorithm[PU、VU]<br/>
        2.policy iteration algorithm[PE、PI]<br/>
        <img src="value and policy itreation.png"/><br/>
        3.truncated policy iteration algorithm<br/>
        <img src="truncated policy iteration.png"/><br/>
    </P>
    <h2 id="5th">第5课</h2>
    <p>
    1.model-based、model-free<br/>
    2.Monte Carlo estimation<br/>
    3.MC Basic<br/>
    <img src="MC Basic.png"/><br/>
    4.MC Exploring Starts、generalized policy iteration(GPI)<br/>
    <img src="MC Exploring Starts.png"/><br/>
    5.soft policy、MC Epsilon-Greedy、exploitation(开发)、exploration<br/>
    <img src="MC Epsilon-Greedy.png"/><br/>
    </P>
    <h2 id="6th">第6课</h2>
    <p>
    1.Stochastic Approximation<br/>
    2.Robbins-Monro<br/>
    <img src="RM.png"/><br/>
    <img src="SGD summary.png"/><br/>
    3.gradient descent(GD)<br/>
    batch gradient descent(BGD)<br/>
    Stochastic gradient descent(SGD)<br/>
    mini-batch gradient descent(MBGD)<br/>
    <img src="MBGD.png"/><br/>
    </P>
    <h2 id="7th">第7课</h2>
    <p>
    1.TD algorithm<br/>
    <img src="TD algorithm.png"/><br/>
    <img src="TD properties.jpg"/><br/>
    2.Sarsa<br/>
    <img src="Sarsa.png"/><br/>
    <img src="Sarsa pseudocode.png"/><br/>
    3.n-step Sarsa<br/>
    <img src="n-step sarsa.png"/><br/>
    4.Q-learning、on-policy、off-policy<br/>
    <img src="Q-learning.png"/><br/>
    <img src="on-policy Q-learning.png"/><br/>
    <img src="off-policy Q-learning.png"/><br/>
    5.summary<br/>
    <img src="TD summary.png"/><br/>
    <img src="TD BE.png"/><br/>
    </P>
</h2>
</html>