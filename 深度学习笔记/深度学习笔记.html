<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>深度学习原理笔记</title>
</head>
<body>
    <h1>目录</h1>
    <a href="https://www.bilibili.com/video/BV1sd4y167NS?p=2&vd_source=8814c55673559083f0880c3918024708">视频资料</a><br/>
    <a href="Book-Mathmatical-Foundation-of-Reinforcement-Learning-Lecture slides 2023.02.20.pdf">PPT</a><br/>
    <a href="强化学习的数学原理书籍2023.08.pdf">参考书</a><br/>
    <h2><a href="#0zero">第0课</a></h2>
    <h2><a href="#1st">第1课</a></h2>
    <h2><a href="#2nd">第2课</a></h2>
    <h2><a href="#3rd">第3课</a></h2>
    <h2><a href="#4th">第4课</a></h2>
    <h2><a href="#5th">第5课</a></h2>
    <h2><a href="#6th">第6课</a></h2>
    <h2><a href="#7th">第7课</a></h2>
    <h2><a href="#8th">第8课</a></h2>
    <h2><a href="#9th">第9课</a></h2>
    <h2><a href="#10th">第10课</a></h2>
    <h2 id="0zero">第0课</h2>
    <p>
        <img src="src/zero.png"/>
    </p>
    <h2 id="1st">第1课</h2>
    <br>
        1.policy、Π<br/>
        2.reward、trajectory(轨迹)、return<br/>
        3.discount return、episode(trail)、[episodic and continuing tasks]<br/>
        4.MDP:Sets[S,A(s),R(s,a)]、[p(s'|s,a)、p(r|s,a)]<br/>
              Policy[Π(a,s)]<br/>
              Markov/Memoryless <br/>
    </p>
    <h2 id="2nd">第2课</h2>
    <p>
        1.boostrapping、v=r+γPv<br/>
        2.state value<br/>
        3.bellman equation<br/>
        <img src="src/bellman equation.png"/><br/>
        4.state transition matrix<br/>
        <img src="src/bellman matrix.png"/><br/>
        5.action value<br/>
        <img src="src/action value.png"/><br/>
    </p>
    <h2 id="3rd">第3课</h2>
    <p>
        1.BOE<br/>
        <img src="src/bellman optimality equation.png"/><br/>
        <img src="src/BOE matrix.png"/><br/>
        2.Fixed point、contraction mapping、contraction mapping theorem<br/>
        <img src="src/contraction mapping.png"/><br/>
        3.value iteration algorithm<br/>
    </p>
    <h2 id="4th">第4课</h2>
    <P>
        1.value iteration algorithm[PU、VU]<br/>
        2.policy iteration algorithm[PE、PI]<br/>
        <img src="src/value and policy itreation.png"/><br/>
        3.truncated policy iteration algorithm<br/>
        <img src="src/truncated policy iteration.png"/><br/>
    </P>
    <h2 id="5th">第5课</h2>
    <p>
    1.model-based、model-free<br/>
    2.Monte Carlo estimation<br/>
    3.MC Basic<br/>
    <img src="src/MC Basic.png"/><br/>
    4.MC Exploring Starts、generalized policy iteration(GPI)<br/>
    <img src="src/MC Exploring Starts.png"/><br/>
    5.soft policy、MC Epsilon-Greedy、exploitation(开发)、exploration<br/>
    <img src="src/MC Epsilon-Greedy.png"/><br/>
    </P>
    <h2 id="6th">第6课</h2>
    <p>
    1.Stochastic Approximation<br/>
    2.Robbins-Monro<br/>
    <img src="src/RM.png"/><br/>
    <img src="src/SGD summary.png"/><br/>
    3.gradient descent(GD)<br/>
    batch gradient descent(BGD)<br/>
    Stochastic gradient descent(SGD)<br/>
    mini-batch gradient descent(MBGD)<br/>
    <img src="src/MBGD.png"/><br/>
    </P>
    <h2 id="7th">第7课</h2>
    <p>
    1.TD algorithm<br/>
    <img src="src/TD algorithm.png"/><br/>
    <img src="src/TD properties.jpg"/><br/>
    2.Sarsa<br/>
    <img src="src/Sarsa.png"/><br/>
    <img src="src/Sarsa pseudocode.png"/><br/>
    3.n-step Sarsa<br/>
    <img src="src/n-step sarsa.png"/><br/>
    4.Q-learning、on-policy、off-policy<br/>
    <img src="src/Q-learning.png"/><br/>
    <img src="src/on-policy Q-learning.png"/><br/>
    <img src="src/off-policy Q-learning.png"/><br/>
    5.summary<br/>
    <img src="src/TD summary.png"/><br/>
    <img src="src/TD BE.png"/><br/>
    </P>
    <h2 id="8th">第8课</h2>
    <p>
        1.value function approximation<br/>
        2.连续空间、离散空间、离散化<br/>
        <img src="src/function approximation.png"/><br/>
        <img src="src/summary function approximation.png"/><br/>
        <img src="src/Projected Bellman error.png"/><br/>
        3.Sarsa with function approximation<br/>
        <img src="src/Sarsa with function approximation.png"/><br/>
        <img src="src/Pseudocode Sarsa with function approximation.png"/><br/>
        4.Q-learning with function approximation<br/>
        <img src="src/Q-learning with function approximation.png"/><br/>
        <img src="src/on policy Q-learning with function approximation.png"/><br/>
        5.deep Q-learning (DQN)、replay buffer<br/>
        <img src="src/DQN.png"/><br/>
        <img src="src/DQN1.png"/><br/>
        <img src="src/deep Q-learning off policy.png"/><br/>
    </P>
    <h2 id="9th">第9课</h2>
    <p>
        1.value based、policy based<br/>
        2.two metrics:average state value、average one-step reward<br/>
        <img src="src/two metrics.png"><br/>
        <img src="src/basic d-pai.png"/><br/>
        <img src="src/~v~r.png"/><br/>
        3.policy function approximation(gradient policy)<br/>
        <img src="src/policy gradient.png"/><br/>
        <img src="src/results of policy gradient.png"/><br/>
        <img src="src/gradient of the metrics.png"/><br/>
        4.softmax functions、REINFORCE<br/>
        <img src="src/softmax functions.png"/><br/>
        <img src="src/REINFORCE.png"/><br/>
        <img src="src/βt.png"/><br/>
        <img src="src/REINFORCE pseudocode.png"/><br/>
    </P>
    <h2 id="10th">第10课</h2>
    <p>
        1.Actor-Critic(AC)<br/>
        <img src="src/gradient of v-pai(s).png"/><br/>
        2.QAC<br/>
        <img src="src/QAC.png"/><br/>
        3.A2C、advantage function<br/>
        <img src="src/baseline.png"/><br/>
        <img src="src/advantage function.png"/><br/>
        <img src="src/A2C.png"/><br/>
        4.importance sampling<br/>
        <img src="src/importanc sampling.png"/><br/>
        <img src="src/importance sampling gradient.png"/><br/>
        <img src="src/importanc sampling equation.png"/><br/>
        5.off-policy acto-critic<br/>
        <img src="src/off-policy actor-critic.png"/><br/>
        6.Deterministic acto-critic(DPG)、DDPG<br/>
        <img src="src/DPG.png"/><br/>
        <img src="src/DPG gradient.png"/><br/>
        <img src="src/DPG pseudocode.png"/><br/>
    </p>
</h2>
</html>